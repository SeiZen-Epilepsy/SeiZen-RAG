{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook 2: Pembuatan Embedding dan Indexing ke ChromaDB (Multi-PDF)\n",
    "\n",
    "Tujuan notebook ini adalah untuk:\n",
    "1. Mengambil chunks teks yang telah diproses dari Notebook 1 (dari semua PDF).\n",
    "2. Menginisialisasi model embedding (OpenAI).\n",
    "3. Membuat embedding untuk setiap chunk.\n",
    "4. Menginisialisasi ChromaDB sebagai vector store.\n",
    "5. Menyimpan (mengindeks) chunks beserta embeddingnya ke ChromaDB.\n",
    "6. Melakukan tes pencarian sederhana."
   ],
   "id": "3b22e012f2fd7c71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Import library",
   "id": "9e98c91feab51364"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\")\n",
    "openai_api_key_standard = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not all([azure_openai_endpoint, azure_openai_api_key, azure_openai_embedding_deployment]):\n",
    "    print(\"ERROR: Satu atau lebih variabel environment Azure OpenAI (ENDPOINT, API_KEY, EMBEDDING_DEPLOYMENT_NAME) tidak ditemukan.\")\n",
    "    print(\"Pastikan file .env Anda sudah dikonfigurasi dengan benar untuk Azure.\")\n",
    "else:\n",
    "    print(\"Variabel environment Azure OpenAI berhasil dimuat.\")\n",
    "\n",
    "if not openai_api_key_standard:\n",
    "    print(\"INFO: OPENAI_API_KEY (standar) tidak ditemukan. Ini mungkin dibutuhkan untuk LLM di Notebook 3 jika tidak menggunakan Azure LLM.\")\n",
    "else:\n",
    "    print(\"OpenAI API Key (standar) berhasil dimuat.\")"
   ],
   "id": "e8588e4149bf299e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "chunk_dir = os.path.join(current_dir, \"chunk_files\")\n",
    "chunk_file_names = [file for file in os.listdir(chunk_dir) if file.lower().endswith(\".pkl\")]"
   ],
   "id": "256ebf6e9f92c8f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_chunks = [] # Inisialisasi\n",
    "chunks_file_path_to_load = None # Path ke file pickle yang akan dimuat\n",
    "\n",
    "print(f\"Mencari direktori chunks di: {chunk_dir}\")\n",
    "\n",
    "if os.path.isdir(chunk_dir):\n",
    "    try:\n",
    "        # Dapatkan daftar semua file .pkl di dalam chunk_dir\n",
    "        chunk_file_names = [file for file in os.listdir(chunk_dir) if file.lower().endswith(\".pkl\")]\n",
    "        print(f\"File .pkl yang ditemukan di {chunk_dir}: {chunk_file_names}\")\n",
    "\n",
    "        if chunk_file_names:\n",
    "            # Memuat file .pkl pertama yang ditemukan\n",
    "            file_to_load = chunk_file_names[0]\n",
    "            chunks_file_path_to_load = os.path.join(chunk_dir, file_to_load)\n",
    "            print(f\"Akan mencoba memuat file chunks: {chunks_file_path_to_load}\")\n",
    "        else:\n",
    "            print(f\"Tidak ada file .pkl yang ditemukan di direktori {chunk_dir}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Direktori chunks {chunk_dir} tidak ditemukan saat mencoba listdir.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat mengakses direktori chunks {chunk_dir}: {e}\")\n",
    "else:\n",
    "    print(f\"ERROR: Direktori chunks {chunk_dir} tidak ditemukan.\")\n",
    "\n",
    "\n",
    "if chunks_file_path_to_load and os.path.exists(chunks_file_path_to_load):\n",
    "    try:\n",
    "        with open(chunks_file_path_to_load, \"rb\") as f:\n",
    "            all_chunks = pickle.load(f)\n",
    "        print(f\"Berhasil memuat {len(all_chunks)} chunks dari {chunks_file_path_to_load}\")\n",
    "        if all_chunks:\n",
    "            print(f\"Contoh metadata chunk pertama yang dimuat: {all_chunks[0].metadata}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat memuat chunks dari pickle {chunks_file_path_to_load}: {e}\")\n",
    "        all_chunks = []\n",
    "elif chunks_file_path_to_load:\n",
    "     print(f\"File chunks {chunks_file_path_to_load} terdefinisi tapi tidak ditemukan.\")\n",
    "else:\n",
    "    print(f\"Tidak ada file pickle yang valid untuk dimuat berdasarkan kriteria.\")\n",
    "\n",
    "\n",
    "if not all_chunks:\n",
    "    print(\"\\nPERINGATAN: `all_chunks` kosong atau tidak berhasil dimuat.\")\n",
    "    print(\"Pastikan Notebook 1 (yang sudah diupdate) telah dijalankan dan menyimpan outputnya dengan benar\")\n",
    "    print(f\"di dalam direktori '{chunk_dir}'.\")"
   ],
   "id": "adce3a23b869d107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if all_chunks:\n",
    "    try:\n",
    "        # Inisialisasi tokenizer yang sesuai (cl100k_base untuk model seperti text-embedding-ada-002, text-embedding-3-small)\n",
    "        # Jika Anda menggunakan model embedding yang sangat berbeda, Anda mungkin perlu tokenizer yang berbeda.\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        print(\"Tokenizer 'cl100k_base' berhasil dimuat.\")\n",
    "\n",
    "        total_tokens = 0\n",
    "        for i, chunk_doc in enumerate(all_chunks):\n",
    "            if isinstance(chunk_doc, Document) and hasattr(chunk_doc, 'page_content'):\n",
    "                text_content = chunk_doc.page_content\n",
    "                tokens = tokenizer.encode(text_content)\n",
    "                total_tokens += len(tokens)\n",
    "                # (Opsional) Print token count per chunk untuk debugging\n",
    "                # if i < 5: # Tampilkan untuk 5 chunk pertama\n",
    "                #     print(f\"Chunk {i+1}: {len(tokens)} tokens\")\n",
    "            else:\n",
    "                print(f\"Peringatan: Item ke-{i} dalam all_chunks bukan objek Document yang valid atau tidak memiliki page_content.\")\n",
    "\n",
    "        print(f\"\\nTotal Estimasi Token untuk semua {len(all_chunks)} chunks: {total_tokens} tokens\")\n",
    "\n",
    "        # (Opsional) Estimasi Biaya Sederhana (contoh untuk text-embedding-ada-002)\n",
    "        # Harga dapat berubah, selalu cek pricing resmi OpenAI/Azure.\n",
    "        # Contoh harga: $0.0001 per 1K tokens untuk text-embedding-3-small (per Mei 2024, bisa berbeda di Azure)\n",
    "        # Contoh harga: $0.0004 per 1K tokens untuk text-embedding-ada-002 (OpenAI)\n",
    "        cost_per_1k_tokens_ada_002 = 0.0004\n",
    "        cost_per_1k_tokens_text_embedding_3_small = 0.0001 # Periksa harga Azure yang berlaku untuk deployment Anda\n",
    "\n",
    "        estimated_cost_ada_002 = (total_tokens / 1000) * cost_per_1k_tokens_ada_002\n",
    "        estimated_cost_text_embedding_3_small = (total_tokens / 1000) * cost_per_1k_tokens_text_embedding_3_small\n",
    "\n",
    "        print(f\"Estimasi biaya embedding (text-embedding-ada-002 @ ${cost_per_1k_tokens_ada_002}/1k tokens): ${estimated_cost_ada_002:.6f}\")\n",
    "        print(f\"Estimasi biaya embedding (text-embedding-3-small @ ${cost_per_1k_tokens_text_embedding_3_small}/1k tokens): ${estimated_cost_text_embedding_3_small:.6f}\")\n",
    "        print(\"PERHATIAN: Estimasi biaya ini adalah perkiraan kasar. Selalu periksa harga resmi dari penyedia layanan Anda (OpenAI/Azure).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menghitung token: {e}\")\n",
    "        print(\"Pastikan library 'tiktoken' sudah terinstal (`pip install tiktoken`).\")\n",
    "else:\n",
    "    print(\"Tidak ada chunks untuk dihitung tokennya.\")"
   ],
   "id": "d670f875f3b26d75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embeddings_model = None # Inisialisasi\n",
    "# Hanya lanjut jika konfigurasi Azure ada dan chunks berhasil dimuat\n",
    "if all([azure_openai_endpoint, azure_openai_api_key, azure_openai_embedding_deployment]) and all_chunks:\n",
    "    try:\n",
    "        embeddings_model = AzureOpenAIEmbeddings(\n",
    "            azure_endpoint=azure_openai_endpoint,\n",
    "            openai_api_key=azure_openai_api_key,\n",
    "            azure_deployment=azure_openai_embedding_deployment,\n",
    "            openai_api_version=azure_openai_api_version,\n",
    "            # chunk_size=16 # Opsional: LangChain akan menangani chunking untuk embedding jika teks terlalu panjang\n",
    "                           # Namun, model Azure memiliki batasan jumlah input (misal, 16 untuk text-embedding-ada-002 jika dipanggil langsung)\n",
    "                           # LangChain AzureOpenAIEmbeddings menangani batching secara internal.\n",
    "                           # Untuk text-embedding-3-small, batas input per request adalah 2048 token, dan bisa handle array hingga 2048 input.\n",
    "        )\n",
    "        print(f\"Model Azure OpenAI Embeddings ('{azure_openai_embedding_deployment}') berhasil diinisialisasi.\")\n",
    "\n",
    "        # (Opsional) Tes embedding pada teks kecil\n",
    "        # sample_text = \"Ini adalah teks contoh untuk Azure embedding.\"\n",
    "        # sample_embedding = embeddings_model.embed_query(sample_text)\n",
    "        # print(f\"\\nContoh embedding untuk '{sample_text}':\")\n",
    "        # print(sample_embedding[:10]) # Tampilkan 10 elemen pertama dari vektor embedding\n",
    "        # print(f\"Dimensi embedding: {len(sample_embedding)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menginisialisasi AzureOpenAIEmbeddings: {e}\")\n",
    "        embeddings_model = None\n",
    "else:\n",
    "    print(\"Tidak dapat menginisialisasi model embedding Azure karena konfigurasi environment atau data chunks tidak ada.\")\n"
   ],
   "id": "1fbe6782ee42e1a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vector_store = None # Inisialisasi\n",
    "if embeddings_model and all_chunks: # Pastikan embeddings_model (Azure) berhasil diinisialisasi\n",
    "    # `project_root` sudah didefinisikan di sel sebelumnya\n",
    "    vector_store_dir = os.path.join(project_root, \"vector_store\", \"chroma_db_azure_multi\") # Nama direktori baru untuk Azure\n",
    "    collection_name = \"rag_azure_multi_pdf_collection\" # Nama koleksi baru untuk Azure\n",
    "\n",
    "    print(f\"Akan menggunakan direktori untuk ChromaDB (Azure): {vector_store_dir}\")\n",
    "    print(f\"Nama koleksi (Azure): {collection_name}\")\n",
    "\n",
    "    # OPSI UNTUK MEMBERSIHKAN DATABASE LAMA ( uncomment jika ingin selalu mulai baru)\n",
    "    # HATI-HATI: Ini akan menghapus semua data di `vector_store_dir`!\n",
    "    # clean_start = True # Set ke True untuk menghapus DB lama\n",
    "    # if clean_start and os.path.exists(vector_store_dir):\n",
    "    #     print(f\"PEMBERSIHAN: Menghapus direktori ChromaDB lama: {vector_store_dir}\")\n",
    "    #     try:\n",
    "    #         shutil.rmtree(vector_store_dir)\n",
    "    #         print(\"Direktori ChromaDB lama berhasil dihapus.\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error saat menghapus direktori ChromaDB lama: {e}\")\n",
    "    #\n",
    "    # os.makedirs(vector_store_dir, exist_ok=True) # Pastikan direktori ada\n",
    "\n",
    "    try:\n",
    "        # Cara paling sederhana adalah menggunakan from_documents jika ini adalah proses indexing utama\n",
    "        print(f\"Membuat (atau menimpa) vector store dengan {len(all_chunks)} chunks menggunakan Azure Embeddings...\")\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=all_chunks,\n",
    "            embedding=embeddings_model, # Menggunakan Azure embeddings_model\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=vector_store_dir\n",
    "        )\n",
    "        vector_store.persist() # Penting untuk menyimpan perubahan ke disk\n",
    "        print(\"ChromaDB vector store berhasil dibuat/diperbarui dan di-persist dengan data baru (Azure).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menginisialisasi atau mengisi ChromaDB dengan Azure Embeddings: {e}\")\n",
    "        vector_store = None\n",
    "else:\n",
    "    print(\"Tidak dapat menginisialisasi ChromaDB karena model embedding Azure atau data chunks tidak ada.\")\n"
   ],
   "id": "322acf2eb42a6f83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if vector_store:\n",
    "    query_text = \"what is difference between superviced and self-superviced learning?\" # Ganti dengan query yang relevan dengan isi PDF Anda\n",
    "    print(f\"\\nMelakukan similarity search untuk query: '{query_text}' (menggunakan Azure Embeddings)\")\n",
    "    try:\n",
    "        search_results = vector_store.similarity_search(query_text, k=3)\n",
    "\n",
    "        if search_results:\n",
    "            print(f\"\\nDitemukan {len(search_results)} hasil yang relevan:\")\n",
    "            for i, doc_result in enumerate(search_results):\n",
    "                print(f\"\\n--- Hasil Pencarian {i+1} ---\")\n",
    "                source_file = doc_result.metadata.get('source', 'Tidak diketahui')\n",
    "                page_number = doc_result.metadata.get('page', 'N/A')\n",
    "                print(f\"Sumber: {source_file}, Halaman: {page_number}\")\n",
    "                print(f\"Konten: {doc_result.page_content[:300]}...\")\n",
    "        else:\n",
    "            print(\"Tidak ada hasil yang ditemukan untuk query tersebut.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat melakukan similarity search: {e}\")\n",
    "else:\n",
    "    print(\"Tidak dapat melakukan pencarian karena vector store tidak terinisialisasi.\")"
   ],
   "id": "34fc39d56ab60f7a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
